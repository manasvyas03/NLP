{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase: hello world! this is example the sample\n",
      "Uppercasecase: HELLO WORLD! THIS IS EXAMPLE THE SAMPLE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text=\"Hello World! This is example the sample\"\n",
    "lowercase_text=text.lower()\n",
    "print(\"Lowercase:\", lowercase_text)\n",
    "\n",
    "uppercase_text=text.upper()\n",
    "print(\"Uppercasecase:\", uppercase_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences: ['The Internet of things describes devices with sensors, processing ability, software and other technologies that connect and exchange data with other devices and systems over the Internet or other communications networks.', 'The Internet of things encompasses electronics, communication, and computer science engineering.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"The Internet of things describes devices with sensors, processing ability, software and other technologies that connect and exchange data with other devices and systems over the Internet or other communications networks. The Internet of things encompasses electronics, communication, and computer science engineering.\")\n",
    "sentences = []\n",
    "for sent in doc.sents:\n",
    "    sentences.append(sent.text)\n",
    "print(\"sentences:\",sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences: ['People don’t always have the words to describe what they’re looking for.', 'But they’ll know it when they see it.', 'As they browse Pinterest content (called ‘Pins’), they fine-tune their tastes and find the perfect idea.', 'And that’s not where the journey ends.', 'People take action on the Pins they like the most—saving them for later, clicking for more info and buying the products that they discover.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vyasm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"People don’t always have the words to describe what they’re looking for. But they’ll know it when they see it. As they browse Pinterest content (called ‘Pins’), they fine-tune their tastes and find the perfect idea. And that’s not where the journey ends. People take action on the Pins they like the most—saving them for later, clicking for more info and buying the products that they discover.\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"sentences:\",sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom sentences: ['The quick brown fox jumps over the lazy dog', '\\nContact: John Doe, Phone: 555-1234, Email: john', 'doe@example', 'com and john', 'doe@example', 'co', 'in Mobile: +91 73647 87348', '\\nHello!!! How are you???\\nHello, world! This is a test', ' The price is $20 and the discount is $5 on all color and colors', '\\nOn July 25, 2024, John visited New York', '\\nThere are some other issues with the colour and colours', '\\nTheir price offers are attractive', '\\nMind Blowing! This is sooooo goooood!\\nWoodchuck, woodchuck, Woodchucks, woodchucks, groundhog, Groundhogs', '']\n",
      "custom words: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.', 'Contact:', 'John', 'Doe,', 'Phone:', '555-1234,', 'Email:', 'john.doe@example.com', 'and', 'john.doe@example.co.in', 'Mobile:', '+91', '73647', '87348.', 'Hello!!!', 'How', 'are', 'you???', 'Hello,', 'world!', 'This', 'is', 'a', 'test.', 'The', 'price', 'is', '$20', 'and', 'the', 'discount', 'is', '$5', 'on', 'all', 'color', 'and', 'colors.', 'On', 'July', '25,', '2024,', 'John', 'visited', 'New', 'York.', 'There', 'are', 'some', 'other', 'issues', 'with', 'the', 'colour', 'and', 'colours.', 'Their', 'price', 'offers', 'are', 'attractive.', 'Mind', 'Blowing!', 'This', 'is', 'sooooo', 'goooood!', 'Woodchuck,', 'woodchuck,', 'Woodchucks,', 'woodchucks,', 'groundhog,', 'Groundhogs.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The quick brown fox jumps over the lazy dog.\n",
    "Contact: John Doe, Phone: 555-1234, Email: john.doe@example.com and john.doe@example.co.in Mobile: +91 73647 87348.\n",
    "Hello!!! How are you???\n",
    "Hello, world! This is a test. The price is $20 and the discount is $5 on all color and colors.\n",
    "On July 25, 2024, John visited New York.\n",
    "There are some other issues with the colour and colours.\n",
    "Their price offers are attractive.\n",
    "Mind Blowing! This is sooooo goooood!\n",
    "Woodchuck, woodchuck, Woodchucks, woodchucks, groundhog, Groundhogs.\"\"\"\n",
    "\n",
    "\n",
    "custom_sentences= text.split('.')\n",
    "print(\"custom sentences:\", custom_sentences)\n",
    "\n",
    "custom_words = text.split()\n",
    "print(\"custom words:\", custom_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['Internet', 'things', 'describes', 'devices', 'sensors', ',', 'processing', 'ability', ',', 'software', 'technologies', 'connect', 'exchange', 'data', 'devices', 'systems', 'Internet', 'communications', 'networks', '.', 'Internet', 'things', 'encompasses', 'electronics', ',', 'communication', ',', 'computer', 'science', 'engineering', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vyasm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "filtered_words= []\n",
    "for token in doc:\n",
    "    if not token.is_stop:\n",
    "        filtered_words.append(token.text)\n",
    "print(\"Filtered Words:\", filtered_words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words without punctuation:  ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.', 'Contact:', 'John', 'Doe,', 'Phone:', '555-1234,', 'Email:', 'john.doe@example.com', 'and', 'john.doe@example.co.in', 'Mobile:']\n"
     ]
    }
   ],
   "source": [
    "#Punctuation mark removal using NLTK\n",
    "\n",
    "import string\n",
    "filtered_words = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.', 'Contact:', 'John', 'Doe,', 'Phone:', '555-1234,', 'Email:', 'john.doe@example.com', 'and', 'john.doe@example.co.in', 'Mobile:']\n",
    "words_no_punct = []\n",
    "for word in filtered_words:\n",
    "    if word not in string.punctuation:\n",
    "        words_no_punct.append(word)\n",
    "print(\"words without punctuation: \",words_no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed words: ['alumnu', 'alumni', 'alumna', 'alumna']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words_new = ['alumnus', 'alumni', 'alumna', 'alumnae']\n",
    "\n",
    "stemmed_words= [ stemmer.stem(word) for word in words_new]\n",
    "print(\"stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words: ['alumnus', 'alumnus', 'alumna', 'alumna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vyasm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words= [lemmatizer.lemmatize(word) for word in words_new]\n",
    "print(\"Lemmatized words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word |  Stemmed  |  Lemmatized\n",
      "-------------------------------\n",
      "running  |  run  |  run\n",
      "runs  |  run  |  run\n",
      "implementation  |  implement  |  implementation\n",
      "automatically  |  automat  |  automatically\n",
      "implementing  |  implement  |  implement\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "words=[\"running\", \"runs\", \"implementation\", \"automatically\", \"implementing\"]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"Word |  Stemmed  |  Lemmatized\")\n",
    "print(\"-------------------------------\")\n",
    "for word in words:\n",
    "    print(f\"{word}  |  {stemmer.stem(word)}  |  {lemmatizer.lemmatize(word, pos='v')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags [('running', 'VBG'), ('runs', 'NNS'), ('implementation', 'NN'), ('automatically', 'RB'), ('implementing', 'VBG')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vyasm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#POS Tag\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "print(\"POS Tags\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos Tags: [('The', 'DET'), ('Internet', 'NOUN'), ('of', 'ADP'), ('things', 'NOUN'), ('describes', 'VERB'), ('devices', 'NOUN'), ('with', 'ADP'), ('sensors', 'NOUN'), (',', 'PUNCT'), ('processing', 'NOUN'), ('ability', 'NOUN'), (',', 'PUNCT'), ('software', 'NOUN'), ('and', 'CCONJ'), ('other', 'ADJ'), ('technologies', 'NOUN'), ('that', 'PRON'), ('connect', 'VERB'), ('and', 'CCONJ'), ('exchange', 'NOUN'), ('data', 'NOUN'), ('with', 'ADP'), ('other', 'ADJ'), ('devices', 'NOUN'), ('and', 'CCONJ'), ('systems', 'NOUN'), ('over', 'ADP'), ('the', 'DET'), ('Internet', 'NOUN'), ('or', 'CCONJ'), ('other', 'ADJ'), ('communications', 'NOUN'), ('networks', 'NOUN'), ('.', 'PUNCT'), ('The', 'DET'), ('Internet', 'NOUN'), ('of', 'ADP'), ('things', 'NOUN'), ('encompasses', 'VERB'), ('electronics', 'NOUN'), (',', 'PUNCT'), ('communication', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('computer', 'NOUN'), ('science', 'NOUN'), ('engineering', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "pos_tags=[]\n",
    "for token in doc:\n",
    "    pos_tags.append((token.text, token.pos_))\n",
    "print(\"Pos Tags:\", pos_tags)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequecy: Counter({' ': 10, 'a': 7, 'e': 7, 'n': 5, 'm': 4, 't': 4, 's': 4, 'i': 3, 'B': 2, 'I': 1, ',': 1, 'd': 1, 'r': 1, 'k': 1, 'H': 1, 'V': 1, 'g': 1, 'c': 1, 'T': 1, 'h': 1, 'u': 1, 'l': 1})\n"
     ]
    }
   ],
   "source": [
    "texting=\"I am Batman, Batman is darkness He is Vengence The ultimate\"\n",
    "from collections import Counter\n",
    "word_counts=Counter(texting)\n",
    "print(\"Word Frequecy:\", word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category  encodes_category\n",
      "0      News                 0\n",
      "1    Sports                 2\n",
      "2      News                 0\n",
      "3  Politics                 1\n",
      "4    Sports                 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data={'Category':['News', 'Sports', 'News', 'Politics', 'Sports']}\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "label_encoder=LabelEncoder()\n",
    "df['encodes_category']=label_encoder.fit_transform(df['Category'])\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
